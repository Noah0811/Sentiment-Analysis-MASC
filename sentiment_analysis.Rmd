---
title: "sentiment-analysis"
author: "Jie Chen"
date: "4/5/2020"
output: html_document
---

# Install needed packages

```{r, echo=FALSE}
#install.packages("tidytext")
#install.packages("textdata")
#install.packages("wordcloud")
#install.packages("ggwordcloud")
```

# Data Processing
Include data retrieving and data cleaning.
```{r}
library(tidytext)
library(dplyr)

# Import data
rawdata <- read.csv("twitterMASC.csv")
# Convert data to data frame
rawdata_df <- as.data.frame(rawdata)

# Extract useful data
procdata_df <- subset(rawdata_df, select = c(screen_name, text))
# Drop rows if no text
procdata_df[procdata_df==""]<-NA
procdata_df <- procdata_df %>% na.omit()

# Word segmentation
text_df <- tibble(line = 1:nrow(procdata_df), text = as.character(procdata_df$text))
text_seg <- text_df %>% unnest_tokens(word, text)
# Remove stop words
data("stop_words")
text_seg <- text_seg %>% anti_join(stop_words)
# Remove words: Chinese characters, t.co, http, alphabetical words, numbers
text_seg <- text_seg %>%
  mutate(word = sub("([\u4E00-\u9FA5]+)|([\u4E00-\u9FA5]+)", "", word)) %>%
  mutate(word = sub("t.co|http", "", word)) %>%
  mutate(word = sub("^[a-zA-Z]$", "", word)) %>%
  mutate(word = sub(".*[0-9]+.*", "", word))
# Drop rows if no text
text_seg[text_seg==""]<-NA
text_seg <- text_seg %>% na.omit()

# Word frequency
text_seg %>%
  count(word, sort = TRUE) 

```

# Bipolar sentiment analysis using bing lexicon (by Bing Liu et al.)

```{r}
library(textdata)
library(ggplot2)
library(reshape2)
library(wordcloud)
library(ggwordcloud)
# Load lexicons
nrc <- get_sentiments("nrc")
bing <- get_sentiments("bing")

# Most common positive and negative words in comments
text_seg_bing <- text_seg %>% inner_join(bing)
text_seg_bing_counts <- text_seg_bing %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# Contribution to sentiment

# bing_word_counts %>%
#   group_by(sentiment) %>%
#   top_n(10) %>%
#   ungroup() %>%
#   mutate(word = reorder(word, n)) %>%
#   ggplot(aes(word, n, fill = sentiment)) +
#   geom_col(show.legend = FALSE) +
#   facet_wrap(~sentiment, scales = "free_y") +
#   labs(y = "Contribution to sentiment",
#        x = NULL) +
#   coord_flip()

# Word cloud in sentiments (bing: positive or negative)
text_seg_bing %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "#FF9999"),
                   max.words = 40)

# Word cloud in sentiments (bing: positive or negative) using ggplot/ggwordcloud
ggplot(
  text_seg_bing_counts[c(1:40),],
  aes(
    label = word, size = n,
    x = sentiment, color = sentiment)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 20) +
  scale_x_discrete(breaks = NULL) +
  theme_minimal()
  
# To break the types, use facet_wrap(~sentiment)
ggplot(
  text_seg_bing_counts[c(1:40),],
  aes(
    label = word, size = n,
    x = sentiment, color = sentiment)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 20) +
  scale_x_discrete(breaks = NULL) +
  theme_minimal() + 
  facet_wrap(~sentiment)
```

# Bipolar sentiment analysis using nrc lexicon (by Saif Mohammad and Peter Turney)

```{r}
library(textdata)
library(ggplot2)
library(reshape2)
library(wordcloud)
library(ggwordcloud)

# Get words with emotion labeled
# Because nrc has 2 sentiments (pos, neg) and 8 emotions, the sentiments and emotions overlap
# Drop emotional words
text_seg_nrc_polar <- text_seg %>% 
  inner_join(nrc, by = "word") %>% 
  subset(sentiment == "positive" | sentiment == "negative" )

# Most common emotional words in comments
text_seg_nrc_polar_counts <- text_seg_nrc_polar %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# Word cloud in sentiments (nrc: 8 emotions)
text_seg_nrc_polar %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(max.words = 40)

# Word cloud in sentiments (nrc: 8 emotions) using ggplot/ggwordcloud
ggplot(
  text_seg_nrc_polar_counts[c(1:100),],
  aes(
    label = word, size = n,
    x = sentiment, color = sentiment)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 10) +
  scale_x_discrete(breaks = NULL) +
  theme_minimal()
  
# To break the types, use facet_wrap(~sentiment)
ggplot(
  text_seg_nrc_polar_counts[c(1:100),],
  aes(
    label = word, size = n,
    x = sentiment, color = sentiment)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 8) +
  scale_x_discrete(breaks = NULL) +
  theme_minimal() + 
  facet_wrap(~sentiment)
```

# Multi-dimentional Sentiment analysis using nrc lexicon (by Saif Mohammad and Peter Turney)
```{r}
library(textdata)
library(ggplot2)
library(reshape2)
library(wordcloud)
library(ggwordcloud)

# Get words with emotion labeled
# Because nrc has 2 sentiments (pos, neg) and 8 emotions, the sentiments and emotions overlap
# Drop sentiment words
text_seg_nrc_emotion <- text_seg %>% 
  inner_join(nrc) %>% 
  subset(sentiment != "positive" & sentiment != "negative" )

# Most common emotional words in comments
text_seg_nrc_emotion_counts <- text_seg_nrc_emotion %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# Word cloud in sentiments (nrc: 8 emotions)
text_seg_nrc_emotion %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(max.words = 40)

# Word cloud in sentiments (nrc: 8 emotions) using ggplot/ggwordcloud
ggplot(
  text_seg_nrc_emotion_counts[c(1:100),],
  aes(
    label = word, size = n,
    x = sentiment, color = sentiment)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 10) +
  scale_x_discrete(breaks = NULL) +
  theme_minimal()
  
# To break the types, use facet_wrap(~sentiment)
ggplot(
  text_seg_nrc_emotion_counts[c(1:100),],
  aes(
    label = word, size = n,
    x = sentiment, color = sentiment)) +
  geom_text_wordcloud_area() +
  scale_size_area(max_size = 8) +
  scale_x_discrete(breaks = NULL) +
  theme_minimal() + 
  facet_wrap(~sentiment)
```